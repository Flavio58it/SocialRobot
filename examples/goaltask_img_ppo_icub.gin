# Training & playing the goal task with image observation with agent iCub
# The observation contains image of first person view and internal states (joint pos and velocities) of iCub 
#
# python -m alf.bin.train --root_dir=~/tmp/img_ppo_icub --gin_file=goaltask_img_ppo_icub.gin
# python -m alf.bin.play --root_dir=~/tmp/img_ppo_icub --gin_file=goaltask_img_ppo_icub.gin
# 
# to run this file weights in trainable_weights() and non_trainable_weights() in file alf/tf_agents/tf_agents/networks/sequential_layer.py need be changed to list

include 'common_ppo.gin'
import social_bot
import alf.environments.wrappers
import tf_agents.networks.utils
import alf.utils.common

# environment config
create_environment.env_name="SocialBot-PlayGround-v0"
create_environment.num_parallel_environments=16

PlayGround.agent_type='icub'
PlayGround.tasks=[@GoalTask, @ICubAuxiliaryTask]
PlayGround.use_image_observation=True
PlayGround.image_with_internal_states=True
PlayGround.resized_image_size=(48, 48)
PlayGround.step_time=0.05

suite_socialbot.load.max_episode_steps=200
GoalTask.max_steps=202
GoalTask.sparse_reward=False
GoalTask.distraction_list=[]
GoalTask.reward_weight=10.0
ICubAuxiliaryTask.target='ball'
ICubAuxiliaryTask.agent_init_pos=(1.0, 0)
ICubAuxiliaryTask.agent_pos_random_range=1.0
ICubAuxiliaryTask.reward_weight=1.0

Agent.observation_transformer=@image_scale_transformer
image_scale_transformer.fields=['image']

# algorithm config
CONV_LAYER_PARAMS=((16, 3, 2), (32, 3, 2))

# encode camera sensor data
img/mlp_layers.conv_layer_params=%CONV_LAYER_PARAMS
img/mlp_layers.fc_layer_params=(50,)
img/mlp_layers.activation_fn=@tf.nn.softsign
img/SequentialLayer.layers = @img/mlp_layers()
img_enc = @img/SequentialLayer()

# encode proprioception data
prop/mlp_layers.fc_layer_params=(100,)
prop/mlp_layers.activation_fn=@tf.nn.softsign
prop/SequentialLayer.layers = @prop/mlp_layers()
prop_enc=@prop/SequentialLayer()

combiner/tf.keras.layers.Concatenate.axis=-1

actor/ActorDistributionNetwork.input_tensor_spec=%observation_spec
actor/ActorDistributionNetwork.output_tensor_spec=%action_spec
actor/ActorDistributionNetwork.preprocessing_layers={'image':%img_enc, 'states':%prop_enc}
actor/ActorDistributionNetwork.preprocessing_combiner=@combiner/tf.keras.layers.Concatenate()
actor/ActorDistributionNetwork.fc_layer_params=(50, 25)
actor/ActorDistributionNetwork.activation_fn=@tf.nn.softsign


value/ValueNetwork.input_tensor_spec=%observation_spec
value/ValueNetwork.preprocessing_layers={'image':%img_enc, 'states':%prop_enc}
value/ValueNetwork.preprocessing_combiner=@combiner/tf.keras.layers.Concatenate()
value/ValueNetwork.fc_layer_params=(50, 25)
value/ValueNetwork.activation_fn=@tf.nn.softsign

import alf.networks.stable_normal_projection_network
actor/ActorDistributionNetwork.continuous_projection_net=@StableNormalProjectionNetwork
StableNormalProjectionNetwork.init_means_output_factor=1e-10
StableNormalProjectionNetwork.inverse_std_transform='softplus'
StableNormalProjectionNetwork.scale_distribution=True
StableNormalProjectionNetwork.state_dependent_std=False

# estimated_entropy.assume_reparametrization=True
# Agent.enforce_entropy_target=True

# debug
PPOLoss.check_numerics=True
estimated_entropy.check_numerics=True
TrainerConfig.summarize_action_distributions=True

# training config
TrainerConfig.eval_interval=50
TrainerConfig.mini_batch_size=2048
TrainerConfig.num_updates_per_train_step=2
TrainerConfig.unroll_length=512
TFUniformReplayBuffer.max_length=512

