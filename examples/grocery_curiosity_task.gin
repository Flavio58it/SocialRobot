# Training curiosity task with Agent Learning Framework (Alf) ICMAlgorithm using pure intrinsic reward
# DISPLAY=null python -m alf.bin.main --root_dir=~/tmp/gro_icm --gin_file=grocery_curiosity_task.gin --alsologtostderr --gin_param="GroceryGround.task_name='curiosity'"
# Training with agent iCub:
# DISPLAY=null python -m alf.bin.main --root_dir=~/tmp/gro_icm_icub --gin_file=grocery_curiosity_task.gin --alsologtostderr --gin_param="GroceryGround.task_name='curiosity'" --gin_param="GroceryGround.agent_type='icub'"

import social_bot

# environment config
create_environment.env_name="SocialBot-GroceryGround-v0"
create_environment.num_parallel_environments=16
create_environment.env_load_fn=@suite_socialbot.load

# algorithm config
train_eval.algorithm_ctor=@create_ac_algorithm
create_ac_algorithm.use_rnns=False
create_ac_algorithm.learning_rate=5e-5
create_ac_algorithm.use_icm=1
create_ac_algorithm.actor_fc_layers=(256,128)
create_ac_algorithm.value_fc_layers=(256,128)
create_ac_algorithm.encoding_fc_layers=(256,)

ICMAlgorithm.hidden_size=64
EncodingNetwork.activation_fn=@tf.nn.tanh

# value network layers
ValueNetwork.activation_fn=@tf.nn.tanh

# actor network layers
ActorDistributionNetwork.activation_fn=@tf.nn.tanh
ActorDistributionNetwork.continuous_projection_net=@NormalProjectionNetwork
NormalProjectionNetwork.init_means_output_factor=1e-10
NormalProjectionNetwork.std_bias_initializer_value=0.0

# use only intrinsic reward for training policy
ActorCriticAlgorithm.gradient_clipping=10.0
ActorCriticAlgorithm.extrinsic_reward_coef=0.0
ActorCriticAlgorithm.intrinsic_reward_coef=1.0
ActorCriticLoss.entropy_regularization=0.01
ActorCriticLoss.use_gae=True
ActorCriticLoss.use_td_lambda_return=True

# training config
on_policy_trainer.train.num_iterations=10000000
on_policy_trainer.train.summarize_grads_and_vars=1
on_policy_trainer.train.train_interval=32
on_policy_trainer.train.summary_interval=1
on_policy_trainer.train.use_tf_functions=1
on_policy_trainer.train.checkpoint_interval=50

PolicyDriver.observation_transformer=@image_scale_transformer
train_eval.debug_summaries=1
