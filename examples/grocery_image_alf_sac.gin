# training & playing with Agent Learning Framework (Alf)
# python -m alf.bin.train --root_dir=~/tmp/gro_goal_pioneer_img_sac --gin_file=grocery_image_alf_sac.gin --alsologtostderr

import alf.trainers.off_policy_trainer
import alf.algorithms.sac_algorithm
import alf.utils.common
import social_bot


# environment config
import alf.environments.wrappers
CHANNEL_ORDER='channels_last'
FrameStack.channel_order=%CHANNEL_ORDER
GroceryGround.image_data_format=%CHANNEL_ORDER
tf.keras.layers.Conv2D.data_format=%CHANNEL_ORDER
create_environment.env_name='SocialBot-GroceryGroundImage-v0'
create_environment.env_load_fn=@suite_socialbot.load
suite_socialbot.load.gym_env_wrappers=(@FrameStack,)
create_environment.num_parallel_environments=12
GroceryGround.resized_image_size=(64, 64)

# algorithm config
create_sac_algorithm.actor_fc_layers=(256,)
create_sac_algorithm.critic_fc_layers=(256,)
create_sac_algorithm.actor_learning_rate=6e-4
create_sac_algorithm.critic_learning_rate=6e-4
create_sac_algorithm.alpha_learning_rate=6e-4
SacAlgorithm.target_update_tau=0.005
OneStepTDLoss.td_error_loss_fn=@losses.element_wise_squared_loss

ActorDistributionNetwork.continuous_projection_net=@NormalProjectionNetwork
ActorDistributionNetwork.conv_layer_params=((16, 3, 2), (32, 3, 2))
ValueNetwork.conv_layer_params=((16, 3, 2), (32, 3, 2))
NormalProjectionNetwork.state_dependent_std=True
NormalProjectionNetwork.mean_transform=None
NormalProjectionNetwork.scale_distribution=True
NormalProjectionNetwork.std_transform=@clipped_exp

PolicyDriver.observation_transformer=@image_scale_transformer

# training config
TrainerConfig.initial_collect_steps=2000
TrainerConfig.mini_batch_length=2
TrainerConfig.unroll_length=1
TrainerConfig.mini_batch_size=256
TrainerConfig.num_iterations=10000000
TrainerConfig.summary_interval=1000
TrainerConfig.eval_interval=5000
TrainerConfig.checkpoint_interval=5000
TrainerConfig.num_updates_per_train_step=1
TrainerConfig.summarize_grads_and_vars=0
TrainerConfig.summaries_flush_secs=10
TrainerConfig.clear_replay_buffer=False

TrainerConfig.trainer=@sync_off_policy_trainer
TrainerConfig.algorithm_ctor=@create_sac_algorithm
TrainerConfig.debug_summaries=1
TrainerConfig.evaluate=1

# takes about (1.6 * num_parallel_environments + 7.5) GB memerory, 28GB for 12 parallel envs
TFUniformReplayBuffer.max_length=50000


